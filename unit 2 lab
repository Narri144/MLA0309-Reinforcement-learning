import numpy as np
import random

# Grid size
rows, cols = 4, 4
states = [(i, j) for i in range(rows) for j in range(cols)]

# Actions
actions = ['up', 'down', 'left', 'right']

# Rewards
goal_state = (3, 3)
obstacles = [(1, 1), (2, 2)]

def reward(state):
    if state == goal_state:
        return 10
    elif state in obstacles:
        return -5
    else:
        return -1

# Initialize Q-values and returns
Q = {(s, a): 0 for s in states for a in actions}
returns = {(s, a): [] for s in states for a in actions}

epsilon = 0.2
gamma = 0.9

def next_state(state, action):
    i, j = state
    if action == 'up':
        i = max(i - 1, 0)
    elif action == 'down':
        i = min(i + 1, rows - 1)
    elif action == 'left':
        j = max(j - 1, 0)
    elif action == 'right':
        j = min(j + 1, cols - 1)
    return (i, j)

def choose_action(state):
    if random.random() < epsilon:
        return random.choice(actions)
    else:
        return max(actions, key=lambda a: Q[(state, a)])

# Monte Carlo Control
episodes = 5000

for _ in range(episodes):
    state = (0, 0)
    episode = []

    while state != goal_state:
        action = choose_action(state)
        next_s = next_state(state, action)
        r = reward(next_s)
        episode.append((state, action, r))
        state = next_s

    G = 0
    visited = set()
    for s, a, r in reversed(episode):
        G = gamma * G + r
        if (s, a) not in visited:
            returns[(s, a)].append(G)
            Q[(s, a)] = np.mean(returns[(s, a)])
            visited.add((s, a))

print("Learned Q-values:")
for key in Q:
    print(key, round(Q[key], 2))
